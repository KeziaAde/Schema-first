{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import rdflib\n",
    "import cProfile, pstats\n",
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from owlready2 import *\n",
    "from urllib.request import urlopen\n",
    "from neo4j import GraphDatabase\n",
    "from urllib import parse\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import google gensim model\n",
    "filename = \"file://C:/Users/Kez/OneDrive/Documents/Jacobs/Yr 3/Thesis/Neo4j_Ontology/GoogleNews.bin.gz\" \n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To do\n",
    "add try except"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Comments\n",
    "- Relevant packages to be downloaded: owlready2, neo4j, gensim, nltk (run nltk.download() in your environment), rdflib, spacy, snakeviz, scikit-learn\n",
    "- Download GoogleNews model here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "- All places with #* are above lines that should be un-commented, in case input ontology is already populated\n",
    "- Sample input for users can be found in the cells titled \"Sample input\", above the cell where the function is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to Strom-DSI ontology only: creating classes, adding object properties and saving to new file\n",
    "onto = get_ontology(\"http://test.org/dsi.owl\")\n",
    "\n",
    "with onto:\n",
    "    class Company(Thing):\n",
    "        pass\n",
    "\n",
    "    class Location(Thing):\n",
    "        pass\n",
    "    class Zip(Location):\n",
    "        pass\n",
    "    class City(Location):\n",
    "        pass\n",
    "\n",
    "    class PowerIndex(Thing):\n",
    "        pass\n",
    "    class Dsi365(PowerIndex):\n",
    "        pass\n",
    "    class Dsi(PowerIndex):\n",
    "        pass\n",
    "\n",
    "    class Timestamp(Thing):\n",
    "        pass\n",
    "    class Day(Timestamp):\n",
    "        pass\n",
    "    class StartTime(Timestamp):\n",
    "        pass\n",
    "    class EndTime(Timestamp):\n",
    "        pass\n",
    "        \n",
    "    class has_for_day(ObjectProperty):\n",
    "        domain = [onto.Company]\n",
    "        range = [onto.Day]\n",
    "        \n",
    "    class has_for_zip(ObjectProperty):\n",
    "        domain = [onto.Company]\n",
    "        range = [onto.Zip]\n",
    "\n",
    "    class has_for_city(ObjectProperty):\n",
    "        domain = [onto.Company]\n",
    "        range = [onto.City] \n",
    "        \n",
    "    class has_for_dsi365(ObjectProperty):\n",
    "        domain = [onto.Day]\n",
    "        range = [onto.Dsi365]\n",
    "        \n",
    "    class has_for_timestamp(ObjectProperty):\n",
    "        domain = [onto.Day]\n",
    "        range = [onto.Timestamp]\n",
    "\n",
    "    class has_for_start(ObjectProperty):\n",
    "        domain = [onto.Day]\n",
    "        range = [onto.StartTime] \n",
    "        \n",
    "    class has_for_dsi(ObjectProperty):\n",
    "        domain = [onto.Timestamp]\n",
    "        range = [onto.Dsi] \n",
    "        \n",
    "onto.save(file = \"C:/Users/Kez/OneDrive/Documents/Jacobs/Yr 3/Thesis/Neo4j_Ontology/Strom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update url to get data from now-10 mins to now, if necessary\n",
    "def urlupdate(url):\n",
    "    # Split url string to get from and to keys\n",
    "    initial = parse.urlsplit(url)\n",
    "    parsed = parse.parse_qs(parse.urlsplit(url).query)\n",
    "    dict_url = dict(parse.parse_qsl(parse.urlsplit(url).query))\n",
    "\n",
    "    # Set new times\n",
    "    now = datetime.datetime.now()\n",
    "    before = now - datetime.timedelta(minutes=10)\n",
    "\n",
    "    # Convert new times back to epoch\n",
    "    from_epoch = round((before - datetime.datetime(1970,1,1)).total_seconds() * 1000)\n",
    "    to_epoch = round((now - datetime.datetime(1970,1,1)).total_seconds() * 1000)\n",
    "\n",
    "    # Replace old times at from and to keys with new times\n",
    "    dict_url[\"from\"] = from_epoch \n",
    "    dict_url[\"to\"] = to_epoch\n",
    "\n",
    "    # Re-encode url string and return it\n",
    "    new_parsed = parse.urlencode(dict_url)\n",
    "    new_initial = initial._replace(query = new_parsed)\n",
    "    newurl = parse.urlunsplit(new_initial)\n",
    "    \n",
    "    return(newurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day(url):\n",
    "    # Split url string to get from and to keys\n",
    "    initial = parse.urlsplit(url)\n",
    "    parsed = parse.parse_qs(parse.urlsplit(url).query)\n",
    "    dict_url = dict(parse.parse_qsl(parse.urlsplit(url).query))\n",
    "\n",
    "    # Get value at date\n",
    "    day = str(dict_url[\"date\"])\n",
    "    \n",
    "    return(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get key, value dataframe from any-level-nested json\n",
    "def loopfunc(y):\n",
    "    # General list declared\n",
    "    pairs = []\n",
    "    def loop(x):\n",
    "        # Enter dictionary and check type of value\n",
    "        for key, val in x.items():\n",
    "            \n",
    "            # If value is dictionary, call function with the value as dictionary\n",
    "            if isinstance(val, dict): \n",
    "                loop(val)\n",
    "                \n",
    "            # If value is iterable, go into each value in iterable\n",
    "            elif isinstance(val, list) or isinstance(val, set):\n",
    "                for subval in val:\n",
    "                    \n",
    "                    # If value is dictionary, loop function with value as dictionary\n",
    "                    if isinstance(subval, dict): \n",
    "                        loop(subval)\n",
    "                    \n",
    "                    # If no, append key and value\n",
    "                    else:\n",
    "                        pairs.append([key, subval])\n",
    "            \n",
    "            # If value is a single item and non-dictionary, append key and value\n",
    "            else:\n",
    "                pairs.append([key, val])\n",
    "    loop(y)\n",
    "    \n",
    "    # Convert general list to dataframe and return dataframe \n",
    "    flat_json = pd.DataFrame(pairs, columns=[\"column\", \"value\"])\n",
    "    \n",
    "    return(flat_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input, prepare data \n",
    "def source_import():  \n",
    "    source = input(\"Source data link: \")  \n",
    "    \n",
    "    # Update url if necessary\n",
    "    # url_obj = urlupdate(source)\n",
    "\n",
    "    # Update url, open and load into dictionary and return dataframe\n",
    "    response = requests.get(source)\n",
    "    file = response.json() \n",
    "    data = loopfunc(file)\n",
    "    \n",
    "    # Specific for Strom\n",
    "    # Get date from url\n",
    "    day = get_day(source)\n",
    "    \n",
    "    # Remove non-alphanumeric characters from data keys, add underscore in spaces and make lowercase\n",
    "    data[\"column\"] = data[\"column\"].str.split(\".\").str[-1].replace(r\"([A-Z])\", r\" \\1\", \n",
    "                                                                    regex = True).replace(\"[^A-Za-z0-9 ]\", \"\", regex=True)    \n",
    "    data[\"column\"] = data[\"column\"].str.strip().str.lower()\n",
    "    \n",
    "    # Write company and day to dataframe\n",
    "    data.loc[data.shape[0]] = [\"company\", \"STROMDAO\"]\n",
    "    data.loc[data.shape[0]] = [\"day\", day]\n",
    "    \n",
    "    print(\"Data imported\")\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Sample input\n",
    "Target ontology: C:/Users/Kez/OneDrive/Documents/Jacobs/Yr 3/Thesis/Neo4j_Ontology/Strom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ontology file: C:/Users/Kez/OneDrive/Documents/Jacobs/Yr 3/Thesis/Neo4j_Ontology/Strom\n"
     ]
    }
   ],
   "source": [
    "# Read ontology, prepare data\n",
    "ontoin = input(\"Target ontology file: \")\n",
    "onto = get_ontology(f\"file://{ontoin}\").load()\n",
    "\n",
    "def onto_import(frame):\n",
    "    # Remove ontology prefix from classes list\n",
    "    ontolist = [str(i).split(\".\")[1] for i in list(onto.classes())]\n",
    "    \n",
    "    #* Get first instance from each class for data matcher and remove ontology name \n",
    "    #instance_list = [str(i).split(\".\")[1] for j in ontolist for i in \n",
    "                     #list(str(onto.search_one(iri = f\"*{j}\").instances())[0])]\n",
    "    \n",
    "    # Add lists into source frame\n",
    "    frame[\"ontoclass\"] = pd.Series(ontolist)\n",
    "    \n",
    "    #* If instance_list: frame[\"ontoclass\"] = pd.Series(ontolist), frame[\"instance\"] = pd.Series(instance_list)\n",
    "    \n",
    "    # Remove non-alpha characters from data keys, add underscore in spaces and make lowercase \n",
    "    frame[\"class\"] = frame[\"ontoclass\"].str.split(\".\").str[-1].replace(r\"([A-Z])\", r\" \\1\", \n",
    "                                                                       regex = True).replace(\"[^A-Za-z0-9 ]\", \"\", regex=True)\n",
    "    frame[\"class\"] = frame[\"class\"].str.strip().str.lower()\n",
    "    \n",
    "    print(\"Ontology imported\")\n",
    "    return(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema comparing function \n",
    "def schema_matcher(frame):\n",
    "    \n",
    "    # Data matcher (compares data types)\n",
    "    def data_matcher(frame): \n",
    "        # Empty output list declared\n",
    "        filter_list = []\n",
    "\n",
    "        # Check if each value in source data is the same type as class instance\n",
    "        if type(frame[\"value\"]) == type(frame[\"instance\"]):\n",
    "\n",
    "            # If yes, append class name and data key \n",
    "            filter_list.append([frame[\"column\"], frame[\"class\"]])\n",
    "\n",
    "        return(filter_list)\n",
    "    \n",
    "    # Semantic matcher\n",
    "    def semantic_matcher(class_col_list): \n",
    "        # Empty output list created\n",
    "        firstmatch = []\n",
    "        \n",
    "        # Declare class and column positions\n",
    "        for item in class_col_list:\n",
    "            var_column = item[0]\n",
    "            var_class = item[1]\n",
    "            semantic_match = 0\n",
    "            \n",
    "            # If the words are the same, similarity is 1.0\n",
    "            if var_column == var_class:\n",
    "                semantic_match = 1.0\n",
    "            \n",
    "            else:\n",
    "                # Check if class and column are both single words \n",
    "                if \" \" not in var_column and \" \" not in var_class:\n",
    "\n",
    "                    # If single words, then create wordnet instances\n",
    "                    column_list = wn.synsets(var_column)\n",
    "                    class_list = wn.synsets(var_class)\n",
    "\n",
    "                    # Check if words exist in wordnet\n",
    "                    if column_list and class_list:\n",
    "                        col_synonym = column_list[0]\n",
    "                        class_synonym = class_list[0]\n",
    "                        semantic_match = round(col_synonym.wup_similarity(class_synonym), 2)\n",
    "\n",
    "                    # If not, check word2vec for word and calculate word similarity\n",
    "                    else:\n",
    "                        # Handle events where variables not in model\n",
    "                        if var_column in model.key_to_index and var_class in model.key_to_index:\n",
    "                            semantic_match = round(model.similarity(var_column, var_class), 2)\n",
    "\n",
    "                # If class and column are not both single words, use word2vec for phrase similarity \n",
    "                else:\n",
    "                    #Handle trailing words\n",
    "                    if var_column in var_class.split(\" \") or var_class in var_column.split(\" \"):\n",
    "                        semantic_match = 0.8\n",
    "                    \n",
    "                    else:\n",
    "                        # Handle events where variables not in model\n",
    "                        if var_column in model.key_to_index and var_class in model.key_to_index:\n",
    "                            semantic_match = round(model.n_similarity(var_column, var_class), 2)\n",
    "                   \n",
    "            # If similarity passes benchmark score, append column, class and similarity score\n",
    "            if semantic_match >= 0.7:\n",
    "                firstmatch.append([var_column, var_class, semantic_match])\n",
    "         \n",
    "        # Make dataframe from list and return\n",
    "        newframe = pd.DataFrame(firstmatch, columns = [\"matched_column\", \"matched_class\", \"schema_score\"])\n",
    "        \n",
    "        return(newframe)\n",
    "    \n",
    "    #* Call data matcher (to be used as semantic matcher input)    \n",
    "    #class_cols = data_matcher(frame)\n",
    "    \n",
    "    # Instead of data matcher, get list of class-column pairs for semantic matcher input\n",
    "    class_cols = [[i, j] for i in frame[\"column\"].unique() for j in frame[\"class\"] if i == i and j == j]\n",
    "    \n",
    "    # Call semantic matcher\n",
    "    schema_score_frame = semantic_matcher(class_cols)  \n",
    "    \n",
    "    print(\"Schema matcher done\")\n",
    "    return(schema_score_frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparing function\n",
    "def example_matcher(frame):\n",
    "    # Get object property list from ontology and prepare strings\n",
    "    properties = [[re.sub(\"(?<!^)(?=[A-Z])\", \" \", str(i).split(\".\")[1]).lower(), i] \n",
    "                  for i in list(onto.object_properties())]\n",
    "     \n",
    "    # Initialise rdf graph \n",
    "    graph = rdflib.Graph()\n",
    "    \n",
    "    # Initialise spacy\n",
    "    english = spacy.load(\"en_core_web_sm\")\n",
    "    english.Defaults.stop_words |= {\"function\",\"relates\",\"reading\"}\n",
    "    stop_words_spacy = english.Defaults.stop_words\n",
    "    \n",
    "    frame[\"example_score\"] = 0\n",
    "    \n",
    "    for property_pair in properties:\n",
    "        # Get property string\n",
    "        prop = property_pair[0]\n",
    "        \n",
    "        # Get range and prepare string\n",
    "        ontoprop = re.split(r\"[^a-zA-Z]\", str(property_pair[1].range))[-2]\n",
    "        \n",
    "        # Get range and domain class of property\n",
    "        rng = re.sub(r\"([A-Z])\", r\" \\1\", ontoprop).strip().lower()\n",
    "        \n",
    "        # Extract keywords and re-join\n",
    "        words = [word for word in prop.split() if not word in stop_words_spacy]\n",
    "        keywords = \"_\".join(words).title()\n",
    "        \n",
    "        # Get rdf related to property\n",
    "        uri = rdflib.URIRef(f\"http://dbpedia.org/resource/{keywords}\")\n",
    "        graph.parse(uri)\n",
    "\n",
    "        # Use property uri as predcicate \n",
    "        for pred, obj in graph.predicate_objects(uri):\n",
    "            if isinstance(obj, rdflib.term.Literal) and obj.language == \"en\":\n",
    "                \n",
    "                #compare columns to objects\n",
    "                for i in frame[\"matched_column\"]:\n",
    "                    example_score = round(model.n_similarity(i, obj), 2)\n",
    "                \n",
    "                     # If similarity passes benchmark score\n",
    "                    if example_score >= 0.7:\n",
    "                        # Assign scores in data dataframe to respective rows\n",
    "                        frame.at[frame[\"matched_column\"] == i & frame[\"matched_class\"] == rng, \n",
    "                                 \"example_score\"] = example_score     \n",
    "\n",
    "    # Calculate mean overall match score by row\n",
    "    frame[\"match_total\"] = np.where(frame[\"example_score\"] != 0, \n",
    "                                    frame[[\"schema_score\", \"example_score\"]].mean(axis=1), frame[\"schema_score\"])\n",
    "    \n",
    "    print(\"Example matcher done\")\n",
    "    return(frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample input\n",
    "Enter XML file: C:\\Users\\Kez\\OneDrive\\Documents\\Jacobs\\Yr 3\\Thesis\\Neo4j_Ontology\\Strom_mapping.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file: C:\\Users\\Kez\\OneDrive\\Documents\\Jacobs\\Yr 3\\Thesis\\Neo4j_Ontology\\Strom_mapping.xml\n"
     ]
    }
   ],
   "source": [
    "# Map to ontology and add relations\n",
    "relations = input(\"XML file: \")\n",
    "tree = ET.parse(relations)\n",
    "root = tree.getroot()\n",
    "\n",
    "def mapper(frame, frame_two):     \n",
    "    # Filter by overall match mean and handle duplicate matches \n",
    "    frame = frame[frame[\"match_total\"] >= 0.7]\n",
    "    frame = frame.sort_values(\"match_total\", ascending=False).drop_duplicates([\"matched_column\", \"matched_class\"])\n",
    "    frame_array= frame[[\"matched_column\", \"matched_class\"]].to_numpy()\n",
    "    \n",
    "    # Declare list of matches\n",
    "    matches = []\n",
    "    \n",
    "    # Iterate through the dataframe, get respective values\n",
    "    for pair in frame_array:\n",
    "        # Get ontology class in onto dataframe, where refined class is same as class in frame array \n",
    "        frameclass = frame_two.loc[frame_two[\"class\"] == pair[1], \"ontoclass\"].values[0]\n",
    "        \n",
    "        # Get data values in onto dataframe, where refined column name is same as column in frame array \n",
    "        actual_column = frame_two.loc[frame_two[\"column\"] == pair[0], \"column\"].values[0]\n",
    "        values = frame_two.loc[frame_two[\"column\"] == pair[0], \"value\"]\n",
    "        \n",
    "        # Append matches to list\n",
    "        matches.append([actual_column, frameclass])\n",
    "        \n",
    "        # Call the class in the ontology\n",
    "        ontoclass = onto.search_one(iri = f\"*{frameclass}\")\n",
    "        \n",
    "        # instantiate count of instances for each class\n",
    "        count = 0\n",
    "        \n",
    "        # Write values in ontology\n",
    "        for value in values:\n",
    "            ind = ontoclass(f\"{value}\")\n",
    "    \n",
    "            # Iterate through elements in root \n",
    "            for elem in root.iter(\"mapping-element\"):\n",
    "                # Get domain and range classes, and property\n",
    "                propertydomain = elem[0].text\n",
    "                propertyrange = elem[1].text \n",
    "                relation = elem[2].text\n",
    "\n",
    "                # Match newly-written instance to element tree classes\n",
    "                if frameclass == propertydomain:\n",
    "                    # If same as domain, find range class in ontology\n",
    "                    rangeclass = onto.search_one(iri = f\"*{propertyrange}\")\n",
    "\n",
    "                    # Get corresponding class for property range\n",
    "                    onto_rng = frame_two.loc[frame_two[\"ontoclass\"] == propertyrange, \"class\"].values[0]\n",
    "\n",
    "                    # Get corresponding matched column for class\n",
    "                    match_rng = frame.loc[frame[\"matched_class\"] == onto_rng, \"matched_column\"].values[0]\n",
    "\n",
    "                    # Get value at nth occurrence of column \n",
    "                    try:\n",
    "                        rng_value = frame_two.loc[frame_two[\"column\"] == match_rng, \"value\"].values[count]\n",
    "                    except IndexError:\n",
    "                        rng_value = frame_two.loc[frame_two[\"column\"] == match_rng, \"value\"].values[0]\n",
    "                    \n",
    "                    # Write to ontology\n",
    "                    rng_ind = rangeclass(f\"{rng_value}\")\n",
    "\n",
    "                    # Create relation\n",
    "                    ind.relation = [rng_ind]\n",
    "                \n",
    "                elif frameclass == propertyrange:\n",
    "                    # If same as range, find domain class in ontology\n",
    "                    domainclass = onto.search_one(iri = f\"*{propertydomain}\")\n",
    "\n",
    "                    # Get corresponding class for property domain\n",
    "                    onto_dom = frame_two.loc[frame_two[\"ontoclass\"] == propertydomain, \"class\"].values[0]\n",
    "\n",
    "                    # Get corresponding matched column for class\n",
    "                    match_dom = frame.loc[frame[\"matched_class\"] == onto_dom, \"matched_column\"].values[0]\n",
    "                    \n",
    "                    # Get value at nth occurrence of column\n",
    "                    try:\n",
    "                        dom_value = frame_two.loc[frame_two[\"column\"] == match_dom, \"value\"].values[count]\n",
    "                    except IndexError:\n",
    "                        dom_value = frame_two.loc[frame_two[\"column\"] == match_dom, \"value\"].values[0]\n",
    "                    \n",
    "                    # Write to ontology\n",
    "                    dom_ind = domainclass(f\"{dom_value}\")\n",
    "\n",
    "                    # Create relation\n",
    "                    dom_ind.relation = [ind]\n",
    "            \n",
    "            count += 1    \n",
    "    \n",
    "    # Convert list to array\n",
    "    matches_array = np.array(matches)\n",
    "    \n",
    "    print(\"Mapping done\")   \n",
    "    return(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in Neo4j function (Neo4j Desktop must be open beforehand)\n",
    "def map_import():\n",
    "    \n",
    "    # Class for importing file into Neo4j\n",
    "    class AutoImport:\n",
    "        # Initialise session\n",
    "        def __init__(self, uri, user, password):\n",
    "            self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        # Close session\n",
    "        def close(self):\n",
    "            self.driver.close()\n",
    "        # Import file and run session\n",
    "        def autoimport(self, file):\n",
    "            with self.driver.session() as session:\n",
    "                f = f'CALL n10s.rdf.import.fetch(\"file:///{file}\",\"RDF/XML\");'\n",
    "                imp = session.run(f)\n",
    "    \n",
    "    # Get output ontology file and save new data to file\n",
    "    output = root.find(\"output\").text\n",
    "    onto.save(file = f\"{output}\")\n",
    "\n",
    "    try:\n",
    "        # Import into Neo4j\n",
    "        test = AutoImport(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n",
    "        \n",
    "        # Prepare output string for double backslach and no .extension\n",
    "        import_file = output.replace(\"/\", \"\\\\\")\n",
    "        \n",
    "        test.autoimport(import_file) \n",
    "        test.close()\n",
    "        print(\"Importing done, wait a few minutes for Neo4j to display\")\n",
    "\n",
    "    except Exception as V:\n",
    "        print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling all functions\n",
    "def main():\n",
    "    data_frame = source_import()\n",
    "\n",
    "    onto_frame = onto_import(data_frame)\n",
    "\n",
    "    schema_frame = schema_matcher(onto_frame)\n",
    "\n",
    "    example_frame = example_matcher(schema_frame)\n",
    "\n",
    "    actual_array = mapper(example_frame, onto_frame)\n",
    "\n",
    "    map_import()\n",
    "    \n",
    "    return(actual_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample input\n",
    "Source data: https://api.corrently.io/v2.0/delfine/dsi?zip=69168&date=2021-12-12\n",
    "\n",
    "*big data: https://api.corrently.io/v2.0/datadomain/query?db=openems&fields=meter0/ActivePower,meter1/ActivePower,meter2/ActivePower,_sum/ConsumptionActivePower&from=1635951600000&to=1635980400000&edge=1237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data link: https://api.corrently.io/v2.0/delfine/dsi?zip=69168&date=2021-12-12\n",
      "Data imported\n",
      "Ontology imported\n",
      "Schema matcher done\n",
      "Example matcher done\n",
      "Mapping done\n",
      "Importing done, wait a few minutes for Neo4j to display\n",
      " \n",
      "*** Profile stats marshalled to file 'C:\\\\Users\\\\Kez\\\\AppData\\\\Local\\\\Temp\\\\tmpvl_k1flx'. \n",
      "Embedding SnakeViz in this document...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-b4fdeee9-d218-11ec-93fd-b971ae4a0cac' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-b4fdeee9-d218-11ec-93fd-b971ae4a0cac\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/C%3A%5CUsers%5CKez%5CAppData%5CLocal%5CTemp%5Ctmpvl_k1flx\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Profiling performance with snakeviz for visualization\n",
    "%load_ext snakeviz\n",
    "\n",
    "# Calling main function to return actual matches array\n",
    "%snakeviz actual_array = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample input\n",
    "True matches: C:\\Users\\Kez\\OneDrive\\Documents\\Jacobs\\Yr 3\\Thesis\\Neo4j_Ontology\\Strom_true.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True matches text file: C:\\Users\\Kez\\OneDrive\\Documents\\Jacobs\\Yr 3\\Thesis\\Neo4j_Ontology\\Strom_true.txt\n",
      "Result: p=1.000, r=1.000, f=1.000\n"
     ]
    }
   ],
   "source": [
    "# Profiling accuracy for test data\n",
    "# Loading true matches array\n",
    "text_file = input(\"True matches text file: \")\n",
    "true_array = np.loadtxt(text_file, dtype=str)\n",
    "\n",
    "# Convert labels to binary for f-score calculations \n",
    "mlb = MultiLabelBinarizer()\n",
    "true_match = mlb.fit_transform(true_array)\n",
    "actual_match = mlb.fit_transform(actual_array)\n",
    "\n",
    "# Calculate metrics\n",
    "p = precision_score(true_match, actual_match, average=\"macro\")\n",
    "r = recall_score(true_match, actual_match, average=\"macro\")\n",
    "f = f1_score(true_match, actual_match, average=\"macro\")\n",
    "\n",
    "print(\"Result: p=%.3f, r=%.3f, f=%.3f\" % (p, r, f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
